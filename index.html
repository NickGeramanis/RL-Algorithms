<!DOCTYPE html>
<html lang="en">
<head>
    <meta content="width=device-width, initial-scale=1" name="viewport"/>
    <title>Reinforcement Learning Algorithms</title>
    <link href="style.css" rel="stylesheet">
</head>

<body>
<div class="sidenav">
    <a href="#home">Home</a>
    <a href="#description">Description</a>
    <a href="#gettingStarted">Getting Started</a>
    <a href="#usage">Usage</a>
    <a href="#about">About</a>
</div>
<div class="main">
    <h1 id="home">Reinforcement Learning Algorithms</h1>
    <a href="https://github.com/NickGeramanis/rl-algorithms">
        <img alt="GitHub image" src="images/github.png"
             style="width: 15%; height: 15%"/>
    </a>
    <h2>
        This repository provides an implementation for some popular
        reinforcement learning algorithms that were tested with OpenAI Gym.
    </h2>
    <h3 id="description">Description</h3>
    <p>The following algorithms have been implemented:</p>
    <ul>
        <li>[ ] Policy Iteration</li>
        <li>[ ] Value Iteration</li>
        <li>[X] Monte Carlo</li>
        <li>[X] SARSA</li>
        <li>[X] Q-Learning</li>
        <li>[X] SARSA(&lambda;)</li>
        <li>[X] Q(&lambda;)</li>
        <li>[X] Monte Carlo with Linear Function Approximation</li>
        <li>[X] SARSA with Linear Function Approximation</li>
        <li>[X] Q-Learning with Linear Function Approximation</li>
        <li>[X] SARSA(&lambda;) with Linear Function Approximation</li>
        <li>[X] Q(&lambda;) with Linear Function Approximation</li>
        <li>[ ] REINFORCE</li>
        <li>[ ] Actor-Critic with Eligibility Traces</li>
        <li>[ ] Least-Squares Policy Iteration</li>
        <li>[ ] Deep Q-Learning</li>
    </ul>
    <p>
        The features in Linear Function Approximation methods can be
        constructed
        with the following algorithms:
    </p>
    <ul>
        <li>Polynomials</li>
        <li>Tile Coding</li>
        <li>Radial Basis Functions</li>
        <li>Fourier Basis</li>
    </ul>
    <p>
        Furthermore,
        <quote>discretizer.py</quote>
        implements a method to discretize continuous spaces.
    </p>
    <h3 id="gettingStarted">Getting Started</h3>
    <h4>Prerequisites</h4>
    <p>The following libraries need to be installed:</p>
    <ul>
        <li>NumPy</li>
        <li>OpenAI Gym</li>
    </ul>
    <h4>Installation</h4>
    <p>
        In order to test the algorithms you must import the appropriate
        package.
    </p>
    <p>For example:</p>
    <code>
        import gym
        from src.algorithms.tabular_q_learning import TabularQLearning
        from src.features.discretizer import Discretizer
    </code>
    <h3 id="usage">Usage</h3>
    <p>
        In order to test an algorithm, you must create an instance of it with
        the appropriate arguments.
    </p>
    <p>For example:</p>
    <code>
        env_name = 'MountainCar-v0' <br>
        env = gym.make(env_name) <br>
        <br>
        initial_learning_rate = 0.1 <br>
        learning_rate_steepness = 0.01 <br>
        learning_rate_midpoint = 1500 <br>
        discount_factor = 0.99 <br>
        n_bins = (20, 20) <br>
        discretizer = Discretizer(n_bins, env.observation_space) <br>
        <br>
        tabular_q_learning = TabularQLearning( <br>
        env, learning_rate_midpoint, discount_factor, initial_learning_rate,
        <br>
        learning_rate_steepness, discretizer) <br>
    </code>
    <p>And then execute the
        <quote>train()</quote>
        method:
    </p>
    <code>
        training_episodes = 2000 <br>
        tabular_q_learning.train(training_episodes)
    </code>
    <img alt="Demonstration" src="images/q_learning_mountain_car.gif"/>
    <p>
        Different algorithms require different arguments. See
        <quote>agent.py</quote>
        for more information.
    </p>
    <p>
        Furthermore, some unit tests have been implemented in folder
        <quote>tests</quote>
        to verify the proper functioning of the code.
    </p>
    <h3 id="about">About</h3>
    <h4>Status</h4>
    <p>Under development.</p>
    <h4>License</h4>
    <p>
        Distributed under the GPL-3.0 License. See
        <quote>LICENSE</quote>
        for more information.
    </p>
    <h4>Authors</h4>
    <a href="https://www.linkedin.com/in/nikolaos-geramanis">Nick Geramanis</a>
</div>
</body>
</html>
